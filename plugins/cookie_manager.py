import os
import json
import tempfile
import time
from typing import List, Dict, Optional, Tuple

from yt_dlp import YoutubeDL  # type: ignore

try:
    # Prefer unified DB wrapper
    from .db_wrapper import DB
except Exception:
    from .sqlite_db_wrapper import DB  # fallback


DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data')
COOKIES_TMP_DIR = os.path.join(DATA_DIR, 'cookies_tmp')

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(COOKIES_TMP_DIR, exist_ok=True)


def _now_iso() -> str:
    return time.strftime('%Y-%m-%d %H:%M:%S')


def parse_netscape(text: str) -> List[Dict]:
    cookies: List[Dict] = []
    for line in (text or '').splitlines():
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split('\t')
        if len(parts) != 7:
            # attempt space-separated fallback
            parts = line.split()
        if len(parts) != 7:
            # invalid line, skip
            continue
        domain, flag, path, secure, expiration, name, value = parts
        cookies.append({
            'domain': domain,
            'flag': flag,
            'path': path,
            'secure': secure,
            'expiration': expiration,
            'name': name,
            'value': value,
        })
    return cookies


def parse_json(text: str) -> List[Dict]:
    try:
        data = json.loads(text or '{}')
    except Exception:
        return []
    items: List[Dict] = []
    # Support either dict with 'cookies' or raw list
    if isinstance(data, dict) and 'cookies' in data and isinstance(data['cookies'], list):
        raw = data['cookies']
    elif isinstance(data, list):
        raw = data
    else:
        raw = []
    for c in raw:
        try:
            items.append({
                'domain': c.get('domain', '.youtube.com'),
                'flag': 'TRUE' if not str(c.get('hostOnly', False)).lower() == 'true' else 'FALSE',
                'path': c.get('path', '/'),
                'secure': 'TRUE' if c.get('secure', True) else 'FALSE',
                'expiration': str(c.get('expirationDate', c.get('expires', 0) or 0)).split('.')[0],
                'name': c.get('name', ''),
                'value': c.get('value', ''),
            })
        except Exception:
            continue
    return items


def parse_txt(text: str) -> List[Dict]:
    # Try netscape first
    items = parse_netscape(text)
    if items:
        return items
    # Fallback: simple name=value per line
    cookies: List[Dict] = []
    for line in (text or '').splitlines():
        line = line.strip()
        if not line or line.startswith('#') or '=' not in line:
            continue
        name, value = line.split('=', 1)
        cookies.append({
            'domain': '.youtube.com',
            'flag': 'TRUE',
            'path': '/',
            'secure': 'TRUE',
            'expiration': '0',
            'name': name.strip(),
            'value': value.strip(),
        })
    return cookies


def to_netscape(cookies: List[Dict]) -> str:
    lines = [
        '# Netscape HTTP Cookie File',
        '# Generated by DownloaderYT-V1 cookie_manager',
    ]
    for c in cookies:
        try:
            domain = c['domain']
            flag = c.get('flag', 'TRUE')
            path = c.get('path', '/')
            secure = c.get('secure', 'TRUE')
            expiration = str(c.get('expiration', '0'))
            name = c['name']
            value = c['value']
            lines.append('\t'.join([domain, flag, path, secure, expiration, name, value]))
        except KeyError:
            continue
    return '\n'.join(lines) + '\n'


def import_cookie_text(name: str, text: str, source_type: str = 'auto') -> Tuple[bool, str]:
    """Import cookie text in any of supported formats and store as Netscape in DB."""
    cookies: List[Dict] = []
    # Detect format
    if source_type == 'netscape' or '# Netscape HTTP Cookie File' in (text or ''):
        cookies = parse_netscape(text)
        source_type = 'netscape'
    elif source_type == 'json' or (text or '').strip().startswith('{') or (text or '').strip().startswith('['):
        cookies = parse_json(text)
        source_type = 'json'
    else:
        cookies = parse_txt(text)
        source_type = 'txt'
    if not cookies:
        return False, 'کوکی‌ها قابل‌پارس نیستند یا خالی هستند'
    netscape_text = to_netscape(cookies)
    db = DB()
    try:
        if hasattr(db, 'add_cookie'):
            db.add_cookie(name=name, source_type=source_type, cookie_text=netscape_text, fmt='netscape', status='unknown')
        else:
            # Fallback: store to file when DB methods not available
            fallback_path = os.path.join(DATA_DIR, f'cookie_{int(time.time())}.txt')
            with open(fallback_path, 'w', encoding='utf-8') as f:
                f.write(netscape_text)
        return True, 'کوکی با موفقیت ذخیره شد'
    except Exception as e:
        return False, f'خطا در ذخیره کوکی: {e}'


def _write_temp_cookie_file(netscape_text: str) -> str:
    fd, path = tempfile.mkstemp(prefix='yt_cookie_', suffix='.txt', dir=COOKIES_TMP_DIR, text=True)
    with os.fdopen(fd, 'w', encoding='utf-8') as f:
        f.write(netscape_text)
    return path


def validate_cookie(netscape_text: str, timeout: int = 10) -> bool:
    """Validate cookie by simulating yt-dlp extraction with cookiefile."""
    try:
        path = _write_temp_cookie_file(netscape_text)
        opts = {
            'quiet': True,
            'simulate': True,
            'cookiefile': path,
            'extractor_retries': 0,
            'socket_timeout': timeout,
            'connect_timeout': timeout,
            'no_warnings': True,
            'no_check_certificate': True,
        }
        def _do_extract():
            with YoutubeDL(opts) as ydl:
                return ydl.extract_info('https://www.youtube.com/watch?v=BaW_jenozKc', download=False)
        info = _do_extract()
        try:
            os.unlink(path)
        except Exception:
            pass
        return bool(info and info.get('id'))
    except Exception:
        try:
            os.unlink(path)
        except Exception:
            pass
        return False


def get_rotated_cookie_file(prev_cookie_id: Optional[int] = None) -> Tuple[Optional[str], Optional[int]]:
    """Pick next cookie (not same as prev) and write a temp netscape file, returning (path, id)."""
    db = DB()
    try:
        if hasattr(db, 'get_next_cookie'):
            rec = db.get_next_cookie(prev_cookie_id)
        else:
            rec = None
        if not rec:
            return None, None
        text = rec.get('cookie_text', '')
        path = _write_temp_cookie_file(text)
        return path, rec.get('id')
    except Exception:
        return None, None


def mark_cookie_used(cookie_id: int, success: bool) -> None:
    db = DB()
    if hasattr(db, 'mark_cookie_used'):
        try:
            db.mark_cookie_used(cookie_id, success)
        except Exception:
            pass


def _sanity_check_youtube_cookie(netscape_text: str) -> bool:
    """Basic structural check: ensure key YouTube auth cookies exist.
    Returns True if at least two critical cookies for .youtube.com are present.
    """
    try:
        critical_names = {
            'SAPISID', 'APISID', 'HSID', 'SSID', 'SID', '__Secure-3PSID', '__Secure-3PAPISID', 'CONSENT'
        }
        count = 0
        for line in (netscape_text or '').splitlines():
            if not line or line.startswith('#'):
                continue
            parts = line.split('\t')
            if len(parts) != 7:
                parts = line.split()
            if len(parts) != 7:
                continue
            domain, flag, path, secure, expiration, name, value = parts
            if '.youtube.com' in domain and name in critical_names and value:
                count += 1
            if count >= 2:
                return True
        return False
    except Exception:
        return False


def validate_and_update_cookie_status(cookie_id: int) -> bool:
    db = DB()
    try:
        recs = db.get_cookie_by_id(cookie_id) if hasattr(db, 'get_cookie_by_id') else []
        rec = recs[0] if isinstance(recs, list) and recs else recs
        if not rec:
            return False
        text = rec.get('cookie_text', '')
        # Combine yt-dlp extraction check with structural sanity check
        ok_extract = validate_cookie(text)
        ok_sanity = _sanity_check_youtube_cookie(text)
        final_ok = bool(ok_extract or ok_sanity)
        if hasattr(db, 'update_cookie_status'):
            db.update_cookie_status(cookie_id, 'valid' if final_ok else 'unknown')
        return final_ok
    except Exception:
        return False